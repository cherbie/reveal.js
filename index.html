<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />

	<title>Concurrency Primitives in Action</title>

	<meta name="description" content="Practical applications of concurrency primitives across C++, Rust and Golang." />
	<meta name="author" content="Clayton Herbst" />

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport" content="width=device-width, initial-scale=1.0" />

	<link rel="stylesheet" href="dist/reset.css" />
	<link rel="stylesheet" href="dist/reveal.css" />
	<link rel="stylesheet" href="dist/theme/black.css" id="theme" />
	<link rel="stylesheet" href="dist/custom.css" />

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css" />
</head>

<body>
	<div class="reveal">
		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<section>
				<svg class="w-[400px] mx-auto mb-[100px]" xmlns="http://www.w3.org/2000/svg" id="endava-logo"
					viewBox="0 0 131 44">
					<style>
						.st0 {
							fill: #de411b;
						}

						.st1 {
							fill: #F0F3F3;
						}
					</style>
					<path class="st0"
						d="M15.6 31.4c-2.7 0-4.9-2.2-4.9-4.9V23c0-.3-.2-1.1-.7-1.6-.5-.5-1.5-.7-1.7-.7H4.9c-2.7 0-4.9-2.2-4.9-4.9C0 13.2 2.2 11 4.9 11h3.4s1.1-.1 1.7-.7c.5-.5.7-1.6.7-1.7V5.1c0-2.7 2.2-4.9 4.9-4.9 2.4 0 4.3 1.6 4.8 3.8.1.3.1.6.1 1v3.4c0 .3.2 1.1.7 1.6.7.7 1.7.8 1.7.8h3.4c2.7 0 4.9 2.2 4.9 4.9s-2.2 4.9-4.9 4.9-4.9-2.2-4.9-4.9v-3.5c0-.3-.2-1.1-.7-1.6-.5-.5-1.5-.7-1.6-.7h-6.9s-1.1.1-1.7.7c-.5.5-.7 1.4-.7 1.6v6.9c0 .2.2 1.1.7 1.6.7.7 1.7.7 1.7.7h3.4c2.7 0 4.9 2.2 4.9 4.9-.1 2.9-2.3 5.1-4.9 5.1zm5.7-26.3c0 2.7 2.2 4.9 4.9 4.9h3.4s1 .1 1.7.7c.5.5.7 1.5.7 1.6v7c0 .3-.2 1-.7 1.6-.6.6-1.7.7-1.7.7h-6.8c-.3 0-1.1-.2-1.6-.7-.5-.5-.7-1.3-.7-1.6v-3.5c0-2.7-2.2-4.9-4.9-4.9s-4.9 2.2-4.9 4.9 2.2 4.9 4.9 4.9H19s1 .1 1.7.8c.5.5.7 1.5.7 1.6v3.5c0 .4 0 .6.1 1 .5 2.2 2.4 3.8 4.8 3.8 2.7 0 4.9-2.2 4.9-4.9V23c0-.2.2-1.1.7-1.7.6-.6 1.7-.8 1.7-.7H37c2.7 0 4.9-2.2 4.9-4.9s-2.2-4.9-4.9-4.9h-3.4c-.2 0-1.1-.2-1.7-.7-.5-.5-.7-1.4-.7-1.6V5.1c0-2.7-2.2-4.9-4.9-4.9-2.8.1-5 2.2-5 4.9z">
					</path>
					<path class="st1"
						d="M49.8 35.7H38.3c.4 3.4 2.3 4.8 4.4 4.8 1.5 0 2.7-.5 3.8-1.4l2 2.2c-1.6 1.5-3.4 2.3-6 2.3-4.1 0-7.5-3.3-7.5-9.1 0-6 3.1-9.1 7.6-9.1 4.9 0 7.3 4 7.3 8.7 0 .6-.1 1.2-.1 1.6zm-7.5-7.2c-2.3 0-3.7 1.6-4 4.4h8.2c-.1-2.4-1.3-4.4-4.2-4.4zM62.6 43.2v-9.9c0-3.5-1.4-4.8-3.6-4.8-2.3 0-3.6 1.6-3.6 4.5v10.2H52V25.8h3.4v1.6c.8-1.3 2.6-2 4.4-2 4 0 6.2 2.7 6.2 7.8v10h-3.4zM79.2 43.2v-1.7c-1.3 1.2-2.7 2-4.6 2-3.8 0-6.7-2.7-6.7-9.3 0-6 3.3-8.8 6.9-8.8 1.7 0 3.4.9 4.4 2v-6.7l3.4-1.7v24.3h-3.4zm0-12.3c-.7-1-2.4-2.3-4.1-2.3-2.5 0-3.8 1.8-3.8 5.6 0 4.4 1.4 6.3 3.9 6.3 1.6 0 3-1.1 3.9-2.2.1 0 .1-7.4.1-7.4zM110.4 43.2H107l-6.2-17.4h3.6l3.1 9.4c.5 1.7 1.1 3.3 1.3 4.3.2-1 .6-2.6 1.2-4.3l3-9.4h3.7l-6.3 17.4zM96 43.2v-1.7c-1.3 1.2-2.7 2-4.6 2-3.8 0-6.7-2.7-6.7-9.3 0-6 3.3-8.8 6.9-8.8 1.7 0 3.4.9 4.4 2v-1.6h3.4v17.5H96zm0-12.3c-.7-1-2.4-2.3-4.1-2.3-2.5 0-3.8 1.8-3.8 5.6 0 4.4 1.4 6.3 3.9 6.3 1.6 0 3-1.1 3.9-2.2.1 0 .1-7.4.1-7.4zM127.6 43.2v-1.7c-1.3 1.2-2.7 2-4.6 2-3.8 0-6.7-2.7-6.7-9.3 0-6 3.3-8.8 6.9-8.8 1.7 0 3.4.9 4.4 2v-1.6h3.4v17.5h-3.4zm0-12.3c-.7-1-2.4-2.3-4.1-2.3-2.5 0-3.8 1.8-3.8 5.6 0 4.4 1.4 6.3 3.9 6.3 1.6 0 3-1.1 3.9-2.2v-7.4z">
					</path>
				</svg>
				<h3>Concurrency Primitives in Action</h3>
				<p>
					<small>Presented by
						<a class="!text-[#EC6861]" href="http://cherbie.github.io">Clayton Herbst</a></small>
				</p>
				<aside class="notes" data-markdown>
					Welcome everybody

					- For todays topic, we will be discussing **Concurrency Primitives In Action**
				</aside>
			</section>

			<section>
				<p>
					<small>Presented using</small>
				</p>
				<a href="https://revealjs.com">
					<img src="https://static.slid.es/reveal/logo-v1/reveal-white-text.svg" alt="reveal.js logo" style="
                height: 180px;
                margin: 0 auto 4rem auto;
                background: transparent;
              " class="demo-logo" />
				</a>

				<aside class="notes" data-markdown>
					- You might have noticed the different presentation structure
					- I am experimenting with reveal.js
					- If you like what you see, I would encourage you to give the framework a go as well
				</aside>
			</section>

			<section>
				<h2 class="!text-[#de411b]">Objectives</h2>
				<ul>
					<li>
						Gain working knowledge on how you might solve certain problems in
						a concurrent manner
					</li>
					<li>
						Gain an appreciation for the similarities and differences between
						various libraries
					</li>
					<li>Feed your curiosity in exploring C++, Rust further</li>
				</ul>

				<aside class="notes" data-markdown>
					... these are some of the outcomes I hope to achieve with everyone on call
				</aside>
			</section>

			<section>
				<h2 class="!text-[#de411b]">Agenda</h2>
				<ol>
					<li>Concurrency vs Parallelism</li>
					<li>Concurrency Primitives Walkthrough & Comparison</li>
					<li>Applying Concurrency in Practice</li>
				</ol>

				<aside class="notes" data-markdown>
					This will be the order of proceedings for today.

					1. Firstly will define a few key terms and concepts
					2. Walk through code examples using various concurrency primitives
					3. Discuss how we can improve the robustness of concurrent code through design patterns
				</aside>
			</section>

			<section>
				<h2 class="!bg-[#DE411B] rounded-xl">Disclaimer</h2>

				<div class="flex flex-row justify-center gap-4 h-[72px]">
					<svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="#DE411B" viewBox="0 0 24 24"
						stroke-width="1.5" stroke="currentColor">
						<path stroke-linecap="round" stroke-linejoin="round"
							d="M12 3v17.25m0 0c-1.472 0-2.882.265-4.185.75M12 20.25c1.472 0 2.882.265 4.185.75M18.75 4.97A48.416 48.416 0 0012 4.5c-2.291 0-4.545.16-6.75.47m13.5 0c1.01.143 2.01.317 3 .52m-3-.52l2.62 10.726c.122.499-.106 1.028-.589 1.202a5.988 5.988 0 01-2.031.352 5.988 5.988 0 01-2.031-.352c-.483-.174-.711-.703-.59-1.202L18.75 4.971zm-16.5.52c.99-.203 1.99-.377 3-.52m0 0l2.62 10.726c.122.499-.106 1.028-.589 1.202a5.989 5.989 0 01-2.031.352 5.989 5.989 0 01-2.031-.352c-.483-.174-.711-.703-.59-1.202L5.25 4.971z" />
					</svg>

					<svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="#DE411B" viewBox="0 0 24 24"
						stroke-width="1.5" stroke="currentColor">
						<path stroke-linecap="round" stroke-linejoin="round"
							d="M12 9v3.75m-9.303 3.376c-.866 1.5.217 3.374 1.948 3.374h14.71c1.73 0 2.813-1.874 1.948-3.374L13.949 3.378c-.866-1.5-3.032-1.5-3.898 0L2.697 16.126zM12 15.75h.007v.008H12v-.008z" />
					</svg>

					<svg xmlns="http://www.w3.org/2000/svg" fill="none" stroke="#DE411B" viewBox="0 0 24 24"
						stroke-width="1.5" stroke="currentColor">
						<path stroke-linecap="round" stroke-linejoin="round"
							d="M4.26 10.147a60.436 60.436 0 00-.491 6.347A48.627 48.627 0 0112 20.904a48.627 48.627 0 018.232-4.41 60.46 60.46 0 00-.491-6.347m-15.482 0a50.57 50.57 0 00-2.658-.813A59.905 59.905 0 0112 3.493a59.902 59.902 0 0110.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.697 50.697 0 0112 13.489a50.702 50.702 0 017.74-3.342M6.75 15a.75.75 0 100-1.5.75.75 0 000 1.5zm0 0v-3.675A55.378 55.378 0 0112 8.443m-7.007 11.55A5.981 5.981 0 006.75 15.75v-1.5" />
					</svg>
				</div>

				<aside class="notes" data-markdown>
					- I felt the need to include a disclaimer before getting started
					- Software design is an art form to a certain extent, and with that comes contentious opinions
					- While I don't think I will be overly controversial in this presentation, when you head online or
					reach out for solutions on stack overflow be prepared to be bomborded
				</aside>
			</section>

			<section id="concurrency-vs-parallelism">
				<section>
					<h2 class="!text-[#de411b]">Concurrency vs Parallelism</h2>

					<aside class="notes" data-markdown>
						... let's get started with an understanding of common terminology
					</aside>
				</section>

				<section id="concurrency">
					<h2 class="!text-[#de411b]">Concurrency</h2>

					<div class="flex justify-center">
						<svg class="h-[400px]" version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 614 552"
							width="614" height="552">
							<!-- svg-source:excalidraw -->

							<defs>
								<style class="style-fonts">
									@font-face {
										font-family: "Virgil";
										src: url("https://excalidraw.com/Virgil.woff2");
									}

									@font-face {
										font-family: "Cascadia";
										src: url("https://excalidraw.com/Cascadia.woff2");
									}
								</style>

							</defs>
							<rect x="0" y="0" width="614" height="552" fill="none"></rect>
							<g stroke-linecap="round" transform="translate(154 10) rotate(0 147 50.5)">
								<path
									d="M25.25 0 M25.25 0 C76.16 -0.65, 130.13 -0.86, 268.75 0 M25.25 0 C107.01 1.14, 187.57 2.33, 268.75 0 M268.75 0 C285.72 1.57, 295.04 8.17, 294 25.25 M268.75 0 C287.27 -1.66, 295.76 6.97, 294 25.25 M294 25.25 C295.17 44.08, 292.4 60.78, 294 75.75 M294 25.25 C293.9 40.81, 293.59 55.25, 294 75.75 M294 75.75 C293.39 91.1, 283.89 102.25, 268.75 101 M294 75.75 C294.35 92.12, 286.58 102.7, 268.75 101 M268.75 101 C208.2 101.58, 145.07 100.39, 25.25 101 M268.75 101 C178.8 100.86, 89.88 100.46, 25.25 101 M25.25 101 C9.15 101.3, -1.98 90.74, 0 75.75 M25.25 101 C8.67 99.28, 1 92.39, 0 75.75 M0 75.75 C-0.92 59.54, 1.25 41.86, 0 25.25 M0 75.75 C-0.43 57.89, 0.19 38.6, 0 25.25 M0 25.25 C-0.68 6.72, 9.35 -1.68, 25.25 0 M0 25.25 C0.56 9.25, 8.55 2.24, 25.25 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(262.2416648864746 48) rotate(0 38.75833511352539 12.5)"><text
									x="38.75833511352539" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Program</text></g>
							<g stroke-linecap="round" transform="translate(223 162) rotate(0 73.5 59.5)">
								<path
									d="M92.5 15 M92.5 15 C103.21 24.05, 118.11 35.31, 128.5 45 M92.5 15 C105.47 26.17, 117.06 37.02, 128.5 45 M128.5 45 C147.13 61.57, 147.97 60.57, 128.5 75 M128.5 45 C144.91 60.92, 145.61 61.01, 128.5 75 M128.5 75 C114.97 84.34, 101.96 95.33, 92.5 104 M128.5 75 C119.22 83.36, 109.08 91.48, 92.5 104 M92.5 104 C72.97 120.81, 74.62 117.72, 55.5 104 M92.5 104 C74.83 120.26, 72.53 120.78, 55.5 104 M55.5 104 C44.21 96.7, 31.64 86.22, 18.5 75 M55.5 104 C45.67 95.73, 33.72 88.66, 18.5 75 M18.5 75 C1.52 60.98, 1.25 59.9, 18.5 45 M18.5 75 C-0.98 59.61, -1.76 61.76, 18.5 45 M18.5 45 C27.75 38.58, 34.65 30.01, 55.5 15 M18.5 45 C33.02 33.63, 47.23 21.5, 55.5 15 M55.5 15 C75.05 -0.65, 73.84 -0.43, 92.5 15 M55.5 15 C73.19 0.08, 76.11 1.57, 92.5 15"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(267.8999996185303 209.25) rotate(0 28.350000381469727 12.5)"><text
									x="28.350000381469727" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Start</text></g>
							<g stroke-linecap="round" transform="translate(33 346) rotate(0 51.5 85.5)">
								<path
									d="M25.75 0 M25.75 0 C40.31 1.13, 55.9 0.33, 77.25 0 M25.75 0 C41.84 -0.08, 58.21 0.3, 77.25 0 M77.25 0 C95.93 -0.41, 103.49 7.08, 103 25.75 M77.25 0 C93.57 -1.4, 102.74 10.3, 103 25.75 M103 25.75 C103.77 52.4, 101.3 75.04, 103 145.25 M103 25.75 C104.54 54.43, 103.03 82.61, 103 145.25 M103 145.25 C101.3 160.43, 94.88 169.07, 77.25 171 M103 145.25 C102.19 164, 93.62 169.65, 77.25 171 M77.25 171 C56.9 169.32, 39.63 172.65, 25.75 171 M77.25 171 C58.35 170.9, 38.26 170.57, 25.75 171 M25.75 171 C10.56 170.97, 0.37 162.72, 0 145.25 M25.75 171 C10.11 170.47, -0.81 163.64, 0 145.25 M0 145.25 C-0.83 112.8, -1.01 84.6, 0 25.75 M0 145.25 C0.57 114.1, 0.64 82.27, 0 25.75 M0 25.75 C0.94 8.13, 10.01 -0.62, 25.75 0 M0 25.75 C0.61 9.08, 6.29 1.4, 25.75 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(41.68333435058594 419) rotate(0 42.81666564941406 12.5)"><text
									x="42.81666564941406" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Thread 1</text></g>
							<g stroke-linecap="round" transform="translate(174.5 343.5) rotate(0 53 85.5)">
								<path
									d="M26.5 0 M26.5 0 C47.99 -0.36, 67.46 -0.48, 79.5 0 M26.5 0 C43.38 1.15, 61.42 1.06, 79.5 0 M79.5 0 C95.57 -1.59, 107.86 10.38, 106 26.5 M79.5 0 C98.2 -0.74, 106.39 8.8, 106 26.5 M106 26.5 C106.74 56.96, 106.15 87.69, 106 144.5 M106 26.5 C106.87 57.42, 106.38 89.24, 106 144.5 M106 144.5 C105.06 163.45, 98.63 170.51, 79.5 171 M106 144.5 C105.95 162.88, 99.36 169.46, 79.5 171 M79.5 171 C66.54 172.43, 54.49 171.59, 26.5 171 M79.5 171 C69.03 170.68, 58.96 170.11, 26.5 171 M26.5 171 C8.2 170.5, 1.64 160.21, 0 144.5 M26.5 171 C10.57 172.74, 0.36 162.66, 0 144.5 M0 144.5 C0.65 100.65, 0.07 54.57, 0 26.5 M0 144.5 C1.05 119.41, 0.47 94.67, 0 26.5 M0 26.5 C-1.67 7.34, 7.05 -1.27, 26.5 0 M0 26.5 C-0.72 9.49, 9.33 1.93, 26.5 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(180.2750015258789 416.5) rotate(0 47.224998474121094 12.5)"><text
									x="47.224998474121094" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Thread 2</text></g>
							<g stroke-linecap="round" transform="translate(316.5 341.5) rotate(0 54.5 85.5)">
								<path
									d="M27.25 0 M27.25 0 C44.41 1.91, 63.89 1.72, 81.75 0 M27.25 0 C40.02 -0.06, 50.91 -0.34, 81.75 0 M81.75 0 C99.89 -0.84, 107.68 9.99, 109 27.25 M81.75 0 C101.67 0.05, 110.07 9.48, 109 27.25 M109 27.25 C109.99 57.15, 109.02 88.85, 109 143.75 M109 27.25 C109.5 56.68, 110.06 87.11, 109 143.75 M109 143.75 C107.66 160.91, 98.57 172.56, 81.75 171 M109 143.75 C109.31 163.08, 101.59 171.2, 81.75 171 M81.75 171 C71.14 170.54, 61.34 169.41, 27.25 171 M81.75 171 C66.14 172.2, 53.25 171.6, 27.25 171 M27.25 171 C9.51 172.76, 0.21 160.31, 0 143.75 M27.25 171 C9.98 173.2, 0.22 163.75, 0 143.75 M0 143.75 C1.27 119.14, 0.1 95.25, 0 27.25 M0 143.75 C0.24 118.19, 1.15 93.46, 0 27.25 M0 27.25 C1.68 9.64, 7.55 1.83, 27.25 0 M0 27.25 C2.01 8.69, 9.47 1.26, 27.25 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(324.0833320617676 414.5) rotate(0 46.91666793823242 12.5)"><text
									x="46.91666793823242" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Thread 3</text></g>
							<g stroke-linecap="round" transform="translate(455.5 340.5) rotate(0 54.5 85.5)">
								<path
									d="M27.25 0 M27.25 0 C41.13 0.85, 60.77 1.2, 81.75 0 M27.25 0 C41.47 0.2, 57 -0.24, 81.75 0 M81.75 0 C99.11 0.56, 110.5 7.63, 109 27.25 M81.75 0 C99.78 0.98, 111.18 9.5, 109 27.25 M109 27.25 C109.68 67.75, 107.71 103.23, 109 143.75 M109 27.25 C109.94 63.08, 109.53 97.53, 109 143.75 M109 143.75 C107.99 162.31, 100 169.98, 81.75 171 M109 143.75 C111.05 163.18, 99.53 171.19, 81.75 171 M81.75 171 C69.8 171.89, 54.48 172.34, 27.25 171 M81.75 171 C61.46 170.63, 42.22 169.88, 27.25 171 M27.25 171 C7.52 171.01, -0.17 161.61, 0 143.75 M27.25 171 C8.55 172.27, -0.28 163.79, 0 143.75 M0 143.75 C2.51 107.26, 0.69 72.47, 0 27.25 M0 143.75 C-1.09 114.92, -0.97 86.15, 0 27.25 M0 27.25 C-0.41 8.7, 9.11 -0.3, 27.25 0 M0 27.25 C-1.7 8.94, 7.4 0.94, 27.25 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(463.4916648864746 413.5) rotate(0 46.50833511352539 12.5)"><text
									x="46.50833511352539" y="0" font-family="Virgil, Segoe UI Emoji" font-size="20px"
									fill="#F0F3F3" text-anchor="middle" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Thread 4</text></g>
							<g stroke-linecap="round" transform="translate(10 325) rotate(0 297 108.5)">
								<path
									d="M32 0 M32 0 C222.48 -0.16, 415.01 0.23, 562 0 M32 0 C210.04 -0.53, 387.62 -0.37, 562 0 M562 0 C582.15 1.63, 594.08 9.84, 594 32 M562 0 C584.9 -1.57, 594.27 12.02, 594 32 M594 32 C593.86 84.89, 593.08 139.43, 594 185 M594 32 C595.36 90.47, 594 150.86, 594 185 M594 185 C596 206.64, 581.34 215.04, 562 217 M594 185 C591.83 208.16, 583.27 217.57, 562 217 M562 217 C351.98 217.9, 143.04 218.07, 32 217 M562 217 C389.5 214.94, 216.24 215.19, 32 217 M32 217 C9.54 217.91, 0.39 208.3, 0 185 M32 217 C10.23 214.74, 1.85 208.25, 0 185 M0 185 C1.49 152.61, 1.04 117.13, 0 32 M0 185 C1.47 153.41, 1.19 119.31, 0 32 M0 32 C0.39 8.89, 11.21 0.34, 32 0 M0 32 C-1.56 11.63, 9.01 2.16, 32 0"
									stroke="#F0F3F3" stroke-width="1" fill="none"></path>
							</g>
							<g transform="translate(40 303) rotate(0 55.79166793823242 12.5)"><text x="0" y="0"
									font-family="Virgil, Segoe UI Emoji" font-size="20px" fill="#F0F3F3"
									text-anchor="start" style="white-space: pre;" direction="ltr"
									dominant-baseline="text-before-edge">Concurrency</text></g>
							<g stroke-linecap="round">
								<g transform="translate(293 121) rotate(0 1.1590671327104474 18.260536950798695)">
									<path
										d="M0.8 1.2 C1.21 6.8, 2.21 28.65, 2.56 34.48 M-0.24 0.78 C0.05 6.46, 1.44 30.14, 1.91 35.74"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
								<g transform="translate(293 121) rotate(0 1.1590671327104474 18.260536950798695)">
									<path
										d="M-3.45 20.54 C-3.66 25.91, 0.95 31.16, 0.67 36.48 M-5.09 19.34 C-3.65 23.91, -1.06 28.66, 1.24 36.1"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
								<g transform="translate(293 121) rotate(0 1.1590671327104474 18.260536950798695)">
									<path
										d="M8.51 19.71 C4.62 25.42, 5.53 30.92, 0.67 36.48 M6.87 18.52 C5.17 23.42, 4.61 28.39, 1.24 36.1"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
							</g>
							<mask></mask>
							<g stroke-linecap="round">
								<g transform="translate(298 289) rotate(0 0.6683343177429606 12.885149433691964)">
									<path
										d="M0.9 0.58 C1 5.1, 1.54 21.8, 1.42 25.93 M-0.09 -0.15 C-0.17 4.14, 0.24 20.4, 0.69 24.29"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
								<g transform="translate(298 289) rotate(0 0.6683343177429606 12.885149433691964)">
									<path
										d="M-3.7 13.26 C-1.63 16.12, -0.4 21.13, 0.3 24.69 M-4.75 12.95 C-2.61 16.1, -1.72 18.84, 0.34 23.69"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
								<g transform="translate(298 289) rotate(0 0.6683343177429606 12.885149433691964)">
									<path
										d="M4.84 12.73 C3.79 15.84, 1.9 21.05, 0.3 24.69 M3.79 12.42 C3.73 15.74, 2.43 18.61, 0.34 23.69"
										stroke="#F0F3F3" stroke-width="1" fill="none"></path>
								</g>
							</g>
							<mask></mask>
						</svg>
					</div>

					<aside class="notes" data-markdown>
						- Concurrency is a concept where seemingly multiple computations happen
						simultaneously
						- This is achieved by dividing a program into smaller, independent
						tasks that can be executed in independent of each other.
						- This approach can either:
						- significantly improve the performance of your programs
						- and/or better logically reflect your solution and/or domain
						- Concurrency does not have to physical, it can be logical or virtual
						- language features such as futures, coroutines ... goroutines enable concurrency
					</aside>
				</section>

				<section id="parallelism">
					<h2 class="!text-[#de411b]">Parallelism</h2>

					<div class="flex justify-center">
						<img class="h-[500px]" src="/images/concurrency_vs_parallelism.png"
							alt="concurrency vs parallelism" />
					</div>

					<aside class="notes" data-markdown>
						- Parallelism is a proper subset of concurrency
						- Parallelism is the simultaneous **physical** execution of
						tasks at runtime
						- it generally requires hardware with multiple computing
						resources and thus resides on the hardware layer.

						... in this diagram
						1. context switching that might be performed by a task scheduler managing a threadpool
					</aside>
				</section>

				<section id="threads">
					<h2 class="!text-[#de411b]">Multithreading</h2>

					<p>
						Threads are a way to achieve parallelism.
					</p>

					<p>The goal is to <span class="text-[#EC6861]">maximize</span> the utilization of <span
							class="text-[#EC6861]">CPU cores</span> and to improve the overall performance of the
						program.</p>
					</po>

					<aside class="notes" data-markdown>
						- A thread is the smallest sequence of programmed instructions that can be managed
						independently by a scheduler.
						- Threads within a process share the same data space, which allows them to communicate
						with each other more easily than if they were separate processes.
						- Managing threads effectively is crucial for achieving the benefits of assembly
						concurrency.
						- Each thread has its own set of registers, a stack, and a program counter.
						- It is the kernel / operating systems job to manage these threads, decide when to pause and
						resume each one and ensure they don't interfere with each other
					</aside>
				</section>

				<section id="synchronization">
					<h2 class="!text-[#de411b]">Synchronization</h2>


					<p>Without proper <span class="text-[#EC6861]">synchronization</span>, threads could access and
						modify shared data <span class="text-[#EC6861]">simultaneously</span>, leading to <span
							class="text-[#EC6861]">inconsistencies</span> and <span
							class="text-[#EC6861]">errors</span>.
					</p>

					<aside class="notes" data-markdown>
						- Synchronization is the coordination of threads to ensure that they work together
						properly.
						> ... read slide
						- inconsistent data reads are the result of race conditions resulting from concurrent software
						- these are very difficult to debug
						- We need to be able to mitigate these risks with appropriate synchronization
						- Synchronization can be achieved through various mechanisms such as locks, semaphores,
						and conditional variables.
					</aside>
				</section>

				<section id="communication">
					<h2 class="!text-[#de411b]">Communication</h2>

					<p>Threads often need to <span class="text-[#EC6861]">exchange data</span> or <span
							class="text-[#EC6861]">signals</span> to
						coordinate their work</p>

					<aside class="notes" data-markdown>
						- Communication between threads is another key principle of concurrency.
						- Threads often need to exchange data or signals to coordinate their work.
						- This can be achieved through:
						1. shared memory
						2. message passing
						3. inter-process communication techniques.

						"channels"
					</aside>
				</section>
				<!-- End Concurrency Parallelism -->
			</section>

			<section id="concurrency-primitives">
				<section>
					<h2 class="!text-[#de411b]">Concurrency Primitives</h2>

					<aside class="notes" data-markdown>
						... so let's consider the primitive structures we have at our disposal
					</aside>
				</section>
			</section>

			<section id="atomics">
				<section>
					<h2 class="!text-[#de411b]">Atomic Types</h2>

					<aside class="notes" data-markdown>
						firstly we have _'atomic data types'_

						- Atomics are a particular data type for which access is always atomic ... what does that
						mean?
						- Reading and writing this data type is guaranteed to happen in a single instruction, so
						there’s
						no way for a handler to run “in the middle” of an access.
						- atomic operations are more efficient than mutexes for simple operations like increments
						and decrements
						- They directly manipulate memory without locking
						- You get atomicity "for free" with no extra hardware for aligned memory stores up to the
						size of the data paths between cores, memory, and I/O busses
						- temporary blocking of other requests is needed to keep an atomic transfer atomic
					</aside>
				</section>

				<section data-background-iframe="https://en.cppreference.com/w/cpp/atomic/atomic">
					<aside class="notes" data-markdown>
						this is the c++ reference for atomic data types

						- data types are explicitly defined
						- access to the data relies on specifying a memory order as well as using the provided interface
						to fetch the data value.
					</aside>
				</section>

				<section id="c++-atomic">
					<h4>C++</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="3|7-22|8|11-22|14|25-45|32"><script type="text/template">
#include <iostream>
#include <thread>
#include <atomic>
#include <gtest/gtest.h>

namespace
{
	std::atomic<int> shared_data(0); // Atomic integer for shared data
	const int NUM_THREADS = 5;

	void increment_shared_atomic(int thread_id)
	{
		// atomically increment the shared data
		shared_data.fetch_add(1, std::memory_order_relaxed);

		std::cout << "Thread " << thread_id
					<< " incremented shared_data to " << shared_data.load(std::memory_order_relaxed)
					<< std::endl;

		// simulate some work
		std::this_thread::sleep_for(std::chrono::milliseconds(thread_id));
	}
} // end namespace ::

TEST(AtomicExample, TestIncrementSharedAtomic)
{
	std::thread thread_handles[::NUM_THREADS];

	// Create threads that increment the shared data
	for (int i = 0; i < ::NUM_THREADS; ++i)
	{
		thread_handles[i] = std::thread(::increment_shared_atomic, i);
	}

	// wait for all threads to finish
	for (auto &thread : thread_handles)
	{
		thread.join();
	}

	// print the final value of the shared data
	const auto final_value = ::shared_data.load(std::memory_order_relaxed);
	std::cout << "Final shared_data value: " << final_value << std::endl;

	EXPECT_EQ(final_value, NUM_THREADS);
}
</script></code></pre>
					<aside class="notes" data-markdown>
						- Absent any constraints on a multi-core system, when multiple threads simultaneously read and
						write to several variables, one thread can observe the values change in an order different from
						the order another thread wrote them.
						- `memory_order_relaxed` : Relaxed operation: there are no synchronization or ordering
						constraints imposed on other reads or writes, only this operation's atomicity is guaranteed
					</aside>
				</section>
				<section id="c++-atomic-test">
					<h4>C++ Output</h4>
					<pre><code class="language-bash" data-trim data-noescape data-line-numbers="6-11|9-10"><script type="text/template">
Note: Google Test filter = AtomicExample.TestIncrementSharedAtomic
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from AtomicExample
[ RUN      ] AtomicExample.TestIncrementSharedAtomic
Thread 0 incremented shared_data to 1
Thread 1 incremented shared_data to 2
Thread 2 incremented shared_data to 3
Thread 3 incremented shared_data to 5
Thread 4 incremented shared_data to 5
Final shared_data value: 5
[       OK ] AtomicExample.TestIncrementSharedAtomic (5 ms)
[----------] 1 test from AtomicExample (5 ms total)
3:
[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (5 ms total)
[  PASSED  ] 1 test.
						</script></code></pre>
					<!-- END C++ ATOMIC OUTPUT -->
				</section>
			</section>

			<section id="mutex">

				<section>
					<h2 class="!text-[#de411b]">Mutex</h2>
				</section>

				<section data-background-iframe="https://doc.rust-lang.org/std/sync/struct.Mutex.html">
					<aside class="notes" data-markdown>
						- A mutual exclusion primitive is useful for protecting shared data
						- This mutex will block threads waiting for the lock to become available.
						- a thread will attempt to obtain a _"lock"_ on the shared resource and then return the _"lock"_
						on the resource once the critical section is executed

						### Optional
						- RAII guards (Resource Acquisition Is Initialisation)
						- Refers to the scoped lifetime of a variable
					</aside>
				</section>

				<section id="rust-mutex">
					<h4>Rust</h4>
					<pre><code class="language-rust" data-trim data-noescape data-line-numbers="|3|11-12|19-30|21-22|27-28"><script type="text/template">
#[cfg(test)]
mod tests {
	use std::sync::{Arc, Mutex};
	use std::thread;

	#[test]
	fn test_mutex_count_across_threads() {
		let counter_start = 0;
		let num_threads = 5;

		// create a shared Mutex for the counter
		let counter = Arc::new(Mutex::new(counter_start));

		let mut thread_handles = vec![];

		for i in 0..num_threads {
			let counter = Arc::clone(&counter);

			// Spawn a thread that increments the counter
			let thread_handle = thread::spawn(move || {
				// entering "critical section"
				let mut counter = counter.lock().expect("unable to acquire mutex");

				// simulate some work
				thread::sleep(std::time::Duration::from_millis(i));

				// Increment the counter
				*counter += 1;
				println!("Thread {} incremented the counter to {}", i, *counter);
			});

			thread_handles.push(thread_handle);
		}

		// wait for all threads to finish
		for handle in thread_handles {
			handle.join().unwrap();
		}

		// access the final counter value
		let final_counter = *counter.lock().expect("unable to acquire mutex");
		println!("Final counter value: {}", final_counter);

		// test assertion
		assert_eq!(final_counter, counter_start + num_threads);
	}
}						
					</script></code></pre>

					<aside class="notes" data-markdown>
						- `std::sync::Arc` stands for _"Atomically Reference Counted"_
						- Rust mutex returns a **RAII* MutexGuard.
					</aside>
					<!-- section id="rust-mutex" -->
				</section>

				<section id="rust-mutex-test">
					<h4>Rust Output</h4>

					<pre><code class="language-bash" data-trim data-noescape data-line-numbers><script type="text/template">
test mutex::tests::test_mutex_count_across_threads ...
Thread 0 incremented the counter to 1
Thread 1 incremented the counter to 2
Thread 3 incremented the counter to 3
Thread 2 incremented the counter to 4
Thread 4 incremented the counter to 5
Final counter value: 5
						</script></code></pre>

					<aside class="notes" data-markdown>

					</aside>
				</section>
				<section id="c++-mutex">
					<h4>C++</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="7-26|9-11|14-19"><script type="text/template">
#ifndef _H_MUTEX
#include <mutex>
#include <memory>

namespace ThreadSafe
{
	class SharedData
	{
	private:
		int        m_data = 0;
		std::mutex m_lock;

	public:
		void increment()
		{
			// lock the mutex to protect the critical section
			std::lock_guard<std::mutex> guard(m_lock);
			++m_data; // increment the shared data
		}

		int read()
		{
			std::lock_guard<std::mutex> guard(m_lock);
			return m_data;
		}
	};
} // namespace ThreadSafe

#endif // #ifndef _H_MUTEX
</script></code></pre>
					<aside class="notes" data-markdown>
						- Here is an example of a thread safe data structure we might define in C++

						- Notice a few similarities and differences:
						1. The mutex does **not** wrap the shared resource it is protecting
						2. You do not _need_ to use a RAII guard, the standard library does provide you with the data
						structure
					</aside>
				</section>
				<section id="c++-mutex-test">
					<h4>C++ Test Structure</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="6-39|11|15-27|16-26|17-18"><script type="text/template">
#include <iostream>
#include <thread>
#include <gtest/gtest.h>
#include <examples/mutex.h>

TEST(MutexExample, TestIncrementSharedData)
{
	const int NUM_THREADS = 5;
	std::thread thread_handles[NUM_THREADS];

	auto shared_data = std::make_shared<ThreadSafe::SharedData>();

	for (int id = 0; id < NUM_THREADS; id++)
	{
		thread_handles[id] = std::thread(
			[id, shared_data](){
			// threadsafe increment
			shared_data->increment();

			std::cout << "Thread " << id
					  << " incremented sharedData to "
					  << shared_data->read()
					  << std::endl;

			// simulate some work
			std::this_thread::sleep_for(std::chrono::milliseconds(id)); });
	}

	// wait for all threads to finish
	for (auto &thread_handle : thread_handles)
	{
		thread_handle.join();
	}

	// print the final value of the shared data
	std::cout << "Final sharedData value: " << shared_data->read() << std::endl;

	EXPECT_EQ(shared_data->read(), NUM_THREADS);
}
					</script></code></pre>
					<aside class="notes" data-markdown>
						- In our test we create 5 threads with an increasing sleeping duration
						- We expect the locks acquired during read and write to conflict with each other, however the
						integrity of the shared data to stay intact

						- Here you can see the `shared_pointer`, which is a C++ smart pointer equivalent of the rust
						`Arc` or _'Atomically Reference Counted'_ shared memory block.
						- Thus this example uses memory on the heap instead memory shared on the stack
					</aside>
				</section>
				<section id="c++-mutex-test">
					<h4>C++ Output</h4>
					<pre><code class="language-bash" data-trim data-noescape data-line-numbers="5-11"><script type="text/template">
Note: Google Test filter = MutexExample.TestIncrementSharedData
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from MutexExample
[ RUN      ] MutexExample.TestIncrementSharedData
Thread 0 incremented sharedData to 2
Thread Thread 3 incremented sharedData to 3
Thread 1 incremented sharedData to 5
Thread 4 incremented sharedData to 5
2 incremented sharedData to 5
Final sharedData value: 5
						</script></code></pre>
					<aside class="notes" data-markdown>
						The test output is in disarray!

						- Here you can see the consequences of not synchronizing writes to shared resources. In this
						case stdout is the shared resource.
						- This also highlights the difficulties that can be experienced in debugging concurrent code
					</aside>
					<!-- END C++ MUTEX OUTPUT -->
				</section>
				<!-- END MUTEX -->
			</section>

			<section id="condvar">
				<section>
					<h2 class="!text-[#de411b]">Conditional Variable</h2>
				</section>

				<section data-background-iframe="https://doc.rust-lang.org/std/sync/struct.Condvar.html">
					<aside class="notes" data-markdown>
						- Condition variables represent the ability to block a thread such that it consumes no CPU time
						while waiting for an event to occur.
						- You can use condition variables in combination with mutexes to create
						synchronization mechanisms for managing thread communication and synchronization.
						- A condition variable must always be used together with a mutex
					</aside>
				</section>
				<section id="rust-condvar">
					<h4>Rust</h4>
					<pre><code class="language-rust" data-trim data-noescape data-line-numbers="|3|11|12-13|15-17|19-35|23-24|32-34|37-51|41-42|44-47|53-55"><script type="text/template">
#[cfg(test)]
mod tests {
	use std::sync::{Arc, Condvar, Mutex};
	use std::thread;

	/// In Rust, you can use condition variables (CondVar) in combination with mutexes to
	/// create synchronization mechanisms for managing thread communication and synchronization.
	///
	/// Here's an example of a Rust application using CondVar to coordinate two threads:
	#[test]
	fn test_condvar_consumer_producer_channel() {
		// create shared data protected by a mutex
		let data = Arc::new((Mutex::new(false), Condvar::new()));

		// clone data for the producer and consumer
		let data_producer = Arc::clone(&data);
		let data_consumer = Arc::clone(&data);

		// spawn the producer thread
		let producer_thread_handle = thread::spawn(move || {
			let (lock, cvar) = &*data_producer;

			// lock the mutex
			let mut started = lock.lock().unwrap();

			// perform some work here
			println!("Producer: Starting some work...");

			// simulate work by sleeping
			thread::sleep(std::time::Duration::from_secs(2));

			// signal the consumer that work is done
			*started = true;
			cvar.notify_one();
		});

		// spawn the consumer thread
		let consumer_thread_handle = thread::spawn(move || {
			let (lock, cvar) = &*data_consumer;

			// lock the mutex
			let mut started = lock.lock().unwrap();

			// wait for the producer to finish work
			while !*started {
				started = cvar.wait(started).unwrap();
			}

			// continue with consumer's work
			println!("Consumer: Work completed!");
		});

		// wait for both threads to finish
		producer_thread_handle.join().unwrap();
		consumer_thread_handle.join().unwrap();
	}
}						
					</script></code></pre>

					<aside class="notes" data-markdown>
						- Here's an example of a Rust application using CondVar to coordinate two threads

						### Consumer Thread
						- `cvar.wait()`: Blocks the current thread until this condition variable receives a
						notification.
						- `cvar.wait()`: This function will atomically unlock the mutex specified and **block** the
						_current thread_.
						- Any calls to `notify_one` or `notify_all` which happen logically after the mutex is
						unlocked, are candidates to _"wake this thread"_
						- this is **not** a _spin-lock_
					</aside>
					<!-- section id="rust-condvar" -->
				</section>

				<section id="rust-condvar-test">
					<h4>Rust Output</h4>

					<pre><code class="language-bash" data-trim data-noescape data-line-numbers><script type="text/template">
test condvar::tests::test_condvar_consumer_producer_channel ...
Producer: Starting some work...
Consumer: Work completed!
ok
						</script></code></pre>
					<aside class="notes" data-markdown>
						- In this test we observe that no matter our order of creating the producer and consumer
						threads, the following output will always be observed
					</aside>
				</section>
				<section id="c++-condvar">
					<h4>C++</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="4|10-15|19-39|41-63|23-31|33-34|45-56|49-55|66-73"><script type="text/template">
#include <iostream>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <queue>
#include <gtest/gtest.h>

namespace
{
	// mutex to protect shared data
	std::mutex mtx;
	// condition variable for synchronization
	std::condition_variable cv; 
	// shared data (queue)
	std::queue<int> data;

	const int NUM_ITEMS = 5; // number of items to produce

	inline void producer()
	{
		for (int i = 0; i < NUM_ITEMS; ++i)
		{
			// simulate producing an item
			int item = i;
			{
				std::lock_guard<std::mutex> lock(mtx);

				// add the item to the queue
				data.push(item);
				std::cout << "Produced: " << item << std::endl;
			}

			// notify the consumer that new data is available
			cv.notify_one();

			// simulate some work
			std::this_thread::sleep_for(std::chrono::milliseconds(200));
		}
	}

	inline void consumer()
	{
		for (int i = 0; i < NUM_ITEMS; ++i)
		{
			int item;
			{
				std::unique_lock<std::mutex> lock(mtx);

				// wait until there's data to consume
				cv.wait(lock, []{ return !data.empty(); });

				// get the item from the queue
				item = data.front();
				data.pop();
			}

			// process the item
			std::cout << "Consumed: " << item << std::endl;

			// simulate some work
			std::this_thread::sleep_for(std::chrono::milliseconds(500));
		}
	}
} // namespace ::

TEST(CondvarExample, TestCondvarConsumerProducerChannel)
{
	std::thread producerThread(::producer);
	std::thread consumerThread(::consumer);

	producerThread.join();
	consumerThread.join();
}
</script></code></pre>
					<aside class="notes" data-markdown>
						- The c++ conditional variable example consists of a thread (the consumer) being notified once
						data is available to be consumed from the shared queue.
					</aside>
				</section>
				<section id="c++-condvar-test">
					<h4>C++ Output</h4>
					<pre><code class="language-bash" data-trim data-noescape data-line-numbers="5-16"><script type="text/template">
Note: Google Test filter = CondvarExample.TestCondvarConsumerProducerChannel
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from CondvarExample
[ RUN      ] CondvarExample.TestCondvarConsumerProducerChannel
Produced: 0
Consumed: 0
Produced: 1
Produced: 2
Consumed: 1
Produced: 3
Produced: 4
Consumed: 2
Consumed: 3
Consumed: 4
[       OK ] CondvarExample.TestCondvarConsumerProducerChannel (2521 ms)
						</script></code></pre>
					<aside class="notes" data-markdown>
						- we observe there is a case where the producer thread appends to the queue twice without the
						consumer thread consuming the value.
						- this is due to the difference in thread sleep durations between the producer and consumer
						thread
						- The consumer thread was simply not ready to consume a value, hence the producer thread
						appended to the shared queue
					</aside>
					<!-- END C++ CONDVAR OUTPUT -->
				</section>
				<!-- END CONDVAR -->
			</section>

			<section id="rwlock">
				<section>
					<h2 class="!text-[#de411b]">Read-Write Lock</h2>

					<aside class="notes" data-markdown>
						- This type of lock allows a number of readers or at most one writer at any point in time.
						- The write portion of this lock typically allows modification of the underlying data (exclusive
						access)
						- the read portion of this lock typically allows for read-only access (shared access)

						# Comparison
						- a _Mutex_ does **not** distinguish between readers or writers that acquire the lock
					</aside>

				</section>
				<section data-background-iframe="https://en.cppreference.com/w/cpp/thread/shared_mutex">
					<aside class="notes" data-markdown>
						- poor code structure can lead to deadlock
					</aside>
				</section>
				<section id="c++-rwlock">
					<h4>C++</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="2|7-28|9-11|14-19|21-27"><script type="text/template">
#ifndef _H_SHARED_MUTEX
#include <shared_mutex>
#include <memory>

namespace ThreadSafe
{
	class SharedData
	{
	private:
		int m_data = 0;
		std::shared_mutex m_rwlock;

	public:
		void increment()
		{
			// lock the mutex to protect the critical section
			std::lock_guard<std::shared_mutex> guard(m_rwlock);
			++m_data; // increment the shared data
		}

		int read()
		{
			std::shared_lock<std::shared_mutex> shared_lock(m_rwlock);
			int value = m_data;
			shared_lock.unlock();
			return value;
		}
	};
} // namespace ThreadSafe

#endif // ifndef _H_SHARED_MUTEX
</script></code></pre>
					<aside class="notes" data-markdown>
						- This c++ read/write mutex lock is purposely very similar to the C++ mutex lock example
						- As you can see in practice it effectively is a one-to-one swap with a regular mutex

						## Differences
						- new data structures need to be used to reflect the _'shared'_ nature of the read _resource
						lock_
						- `std::shared_lock` vs `std::lock_guard` vs `std::unique_lock`

						## read()
						- Notice we release the `shared_lock` earlier?
						- This was purely to show how one might need to be conscious of release a _resource lock_ if you
						did not have access to scoped locks
					</aside>
				</section>
				<section id="c++-rwlock-test">
					<h4>C++ Output</h4>
					<pre><code class="language-bash" data-trim data-noescape data-line-numbers="5-12"><script type="text/template">
Note: Google Test filter = SharedMutexExample.TestIncrementRwLockSharedData
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SharedMutexExample
[ RUN      ] SharedMutexExample.TestIncrementRwLockSharedData
Thread 0 incremented sharedData to 1
Thread Thread Thread 4Thread  incremented sharedData to 5
1 incremented sharedData to 5
3 incremented sharedData to 5
2 incremented sharedData to 5
Final sharedData value: 5
[       OK ] SharedMutexExample.TestIncrementRwLockSharedData (5 ms)
						</script></code></pre>
					<aside class="notes" data-markdown>
						- As you can see there is rather useless standard output logs here
						- You are just going to have to take my word for it ... or experiment with this repository
						yourself :man_shrugging:
						- Notice that 5 is logged for most threads?
						- This is likely due to how the system prioritizes locking between shared and unique mutexes.
						- Here it seems the writing thread was able to acquire an outwright lock delaying any reads by
						threads
					</aside>
					<!-- END C++ RWLock OUTPUT -->
				</section>
				<!-- END RWLock -->
			</section>

			<section id="barrier">
				<section>
					<h2 class="!text-[#de411b]">Barrier Synchronization</h2>

					<aside class="notes" data-markdown>
						- The last synchronization primitive we will be discussing is the barrier synchronization
						primitive
						- This might not be as common-place as a mutex or conditional variable however its utility is
						still undisputed
						- At the end of each test we have been waiting for all threads to join with the main application
						process / thread which is good practice as threads spawned from child threads can detach from
						the main process thread potentially consuming system resources even following the main process
						exiting.
						- Effectively joining the threads has behaved as a barrier
					</aside>
				</section>

				<section data-background-iframe="https://doc.rust-lang.org/std/sync/struct.Barrier.html">
					<aside class="notes" data-markdown>
						- a barrier enables multiple threads to synchronize the beginning of some computation.
					</aside>
				</section>
				<section id="rust-barrier">
					<h4>Rust</h4>
					<pre><code class="language-rust" data-trim data-noescape data-line-numbers="3|7|11-12|17-23|14"><script type="text/template">
#[cfg(test)]
mod tests {
	use std::sync::{Arc, Barrier};
	use std::thread;

	#[test]
	fn test_barrier_thread_synchronisation() {
		const NUM_THREADS: usize = 10;
		let mut thread_handles = Vec::with_capacity(NUM_THREADS);

		// create shared barrier across NUM_THREADS
		let barrier = Arc::new(Barrier::new(NUM_THREADS));

		for _ in 0..NUM_THREADS {
			let thread_barrier = Arc::clone(&barrier);

			// the same messages will be printed together.
			// you will NOT see any interleaving.
			let thread_handle = thread::spawn(move || {
				println!("before wait");
				thread_barrier.wait();
				println!("after wait");
			});
			thread_handles.push(thread_handle)
		}

		// gracefully wait for threads to finish.
		for handle in thread_handles {
			handle.join().unwrap();
		}
	}
}						
					</script></code></pre>

					<aside class="notes" data-markdown>
						- the key observation in this test is that there will be no interleaving between the different
						threads standard output
					</aside>
					<!-- section id="rust-barrier" -->
				</section>

				<section id="rust-barrier-test">
					<h4>Rust Output</h4>

					<pre><code class="language-bash" data-trim data-noescape data-line-numbers><script type="text/template">
test barrier::tests::test_barrier_thread_synchronisation ... before wait
before wait
before wait
before wait
before wait
before wait
before wait
before wait
before wait
before wait
after wait
after wait
after wait
after wait
after wait
after wait
after wait
after wait
after wait
after wait
ok
						</script></code></pre>
				</section>
			</section>

			<section id="synchronization-patterns">
				<section>
					<h2 class="!text-[#de411b]">Synchronization Patterns</h2>

					<ul>
						<li><span class="italic">Scoped Locking</span> C++ idiom</li>
						<li><span class="italic">Thread-Safe Interface</span> design pattern</li>
						<li><span class="italic">Double-Checked Locking Optimization</span> design pattern</li>
					</ul>

					<aside class="notes" data-markdown>
						- Next we will be discussing how we can structure our concurrent code in order to improve the
						_maintainability_ and _robustness_ of our code in common use cases.
						- The three patterns we will discuss are the following ...
						- This is where the concurrency topic becomes very interesting and the complexity drastically
						increases.
						- A lot of common, well documented design patterns have their respective concurrent variations
						- I have designed to limit my scope as this really is a topic in itself.

						### Reference:
						- Pattern-Oriented Software Architecture, Volume 2, Patterns for Concurrent and Networked
						Objects
					</aside>
				</section>

				<section>
					<h2 class="!text-[#de411b]">Scoped Locking C++ Idiom</h2>

					<p>Also known as <span class="text-[#b5533c]">RAII (Resource-Acquisition-Is-Initialization)</span>
					</p>

					<aside class="notes" data-markdown>
						## Scoped Locking C++ idiom

						- also known as RAII (Resource-Acquisition-Is-Initialization) or simply a _'Guard'_
						- ensures that a lock is acquired automatically when control enters a scope and released
						automatically when control leaves the scope, regardless of the return path from the scope.
					</aside>
				</section>

				<section>
					<h4 class="!text-[#de411b]">Webserver Code Snippet Example</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers="1|4-5|7-8|10-15|16-23"><script type="text/template">
class Hit_Counter
{
public:
    // Increment the hit count for a URL <path> name.
    bool increment(const std::string &path)
    {
        // Acquire lock to enter critical section.
        lock_.acquire();
        Table_Entry *entry = lookup_or_create(path);
        if (entry == 0)
        {
            // Something’s gone wrong, so bail out.
            lock_.release();
            return false; // Return a ‘failure’ value.
        }
        else
        {
            // Increment hit count for <path> name.
            entry->increment_hit_count();
            // Release lock to leave critical section.
            lock_.release();
            return true;
        }
    }

    // Other public methods omitted…

private:
    // Lookup the table entry that maintains the hit count
    // associated with <path> name, creating the entry if
    // it doesn’t exist.
    Table_Entry *lookup_or_create(const std::string &path);

    // Serialize access to the critical section.
    Thread_Mutex lock_;
};
					</script></code></pre>

					<aside class="notes" data-markdown>
						- I have taken an example provided in the book _'Pattern-Orientated Software Architecture'_.
						- The code snippet is concerned with a webserver maintaining a _'hit count'_ on how many times a
						particular endpoint is accessed.
						- Webservers generally operate in a multithreaded environment to improve system resource
						utilization

						### Example
						- In this example we serialize access to a hit-count component by acquiring and releasing a lock
						explicitly in each public method.
						- This example is slightly more complicated than previous examples considered due to the branch
						in the `increment()` functions flow of control.
						- This is still a very simple use case but notice the heavy reliance on the competence of the
						programmer to not cause a deadlock?
						- Imagine years down the track adding an additional `else if` block?
						- Imagine reviewing this PR with zero context on the implementation details ... would you trust
						yourself to pick up on not introducing a deadlock?
						- Well you shouldn't because there is a better way
					</aside>
				</section>

				<section>
					<h4 class="!text-[#de411b]">Thread_Mutex_Guard.h</h4>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers><script type="text/template">
class Thread_Mutex_Guard
{
public:
    // Store a pointer to the lock and acquire the lock.
    Thread_Mutex_Guard(Thread_Mutex &lock)
        : lock_(&lock), owner_(false)
    {
        lock_->acquire();

        // Only set to true if <acquire> succeeds.
        owner_ = true;
    }

    // Release the lock when the guard goes out of scope.
    ~Thread_Mutex_Guard()
    {
        // Only release the lock if it was acquired
        // successfully, i.e., <false> indicates that
        // <acquire> failed..
        if (owner_)
            lock_->release();
    }

private:
    Thread_Mutex *lock_; // Pointer to our lock.
    bool         owner_;         // Is <lock_> held by this object?

private:
    // Disallow copying or assignment.
    Thread_Mutex_Guard(const Thread_Mutex_Guard &);
    void operator=(const Thread_Mutex_Guard &);
};
</script></code></pre>
					<aside class="notes" data-markdown>
						- let's improve this by defining a mutex guard
					</aside>
				</section>

				<section>
					<pre><code class="language-c++" data-trim data-noescape data-line-numbers><script type="text/template">
class Hit_Counter
{
public:
    // Increment the hit count for a URL <path> name.
    bool increment(const string &path)
    {
        // Use Scoped Locking to acquire and release
        // the <lock_> automatically.
        Thread_Mutex_Guard guard(lock_);

        Table_Entry *entry = lookup_or_create(path);
        if (entry == 0)
        {

            return false; // Destructor releases <lock_>
        }
        else
        {
            // Increment hit count for this <path> name.
            entry->increment_hit_count();
 
            return true; // Destructor releases <lock_>
        }
    }

    // Other public methods omitted.

private:
    // Serialize access to the critical section.
    Thread_Mutex lock_;
};
</script></code></pre>
					<aside class="notes" data-markdown>
						- This example looks a little more familiar to our previous code samples

						### Benefit
						- Use of the mutex guard improves the robustness of the code

						### Liabilities
						1. potential for deadlocks when used recursively
						2. limitations with language specific semantics
					</aside>
				</section>

				<section>
					<h2 class="!text-[#de411b]">Thread-Safe Interface Pattern</h2>

					<aside class="notes" data-markdown>
						## Thread-Safe Interface Pattern
						- This pattern minimizes the locking overhead and ensures that intra-component method calls do not incur
						‘self-deadlock’ by trying to reacquire a lock that a component already holds.
						- When considering library interface design, multi-threaded components often contain multiple
						publicly-accessible interface methods and private implementation methods that can alter the
						component state
						- To prevent race conditions, a lock internal to the component can be used to serialize
						interface method invocations that access its state.
					</aside>
				</section>
				<section>
					<h4 class="!text-[#de411b]">File_Cache.h</h4>

					<pre><code class="language-c++" data-trim data-line-numbers data-noescape><script type="text/template">
template <class LOCK>
class File_Cache
{
public:
	// The following two interface methods just
	// acquire/release the <LOCK> and forward to
	// their corresponding implementation methods.
	const void *lookup(const string &path) const;

	void        insert(const string &path);

private:
	// The following two implementation methods do not
	// acquire/release the <LOCK> and perform the actual
	// work associated with managing the <File_Cache>.
	const void *lookup_i(const string &path) const;

	void        insert_i(const string &path);

	// … Other implementation methods omitted …
};
					</script></code></pre>
					<aside class="notes" data-markdown>
						### Keys concepts:
						1. Thread-safe components should be designed to avoid ‘self-deadlock’.
						2. Thread-safe components should be designed to incur only minimal locking overhead

						### Solution:
						- The solution is to structure all components that process intra-component method invocations according two
						design conventions:
						1. **Interface methods check**: All interface methods, such as C++ public methods, should only
						acquire/release component lock(s), thereby performing synchronization checks at the ‘border’ of
						the component.
						2. **Implementation methods trust**: Implementation methods, such as C++ private and protected
						methods, should only perform work when called by interface methods. They do not need to lock
					</aside>
				</section>

				<section>
					<h4 class="!text-[#de411b]">File_Cache.h</h4>
					<pre><code class="language-c++" data-trim data-line-numbers="5-14,28-46" data-noescape=""><script type="text/template">
template <class LOCK>
class File_Cache
{
public:
    // Return a pointer to the memory-mapped file
    // associated with <path> name, adding it to
    // the cache if it doesn’t exist.
    const void *lookup(const string &path) const
    {
        // Use the Scoped Locking idiom to acquire
        // and release the <lock_> automatically.
        Guard<LOCK> guard(lock_);
        return lookup_i(path);
    }

    // Add <path> name to the file cache.
    void insert(const string &path)
    {
        // Use the Scoped Locking idiom to acquire
        // and release the <lock_> automatically.
        Guard<LOCK> guard(lock_);
        insert_i(path);
    }

private:
    mutable LOCK lock_; // The strategized locking object

    // The following implementation methods do not
    // acquire or release <lock_> and perform their
    // work without calling any interface methods.
    const void *lookup_i(const string &path) const
    {
        const void *file_pointer = check_cache_i(path);
        if (file_pointer == 0)
        {
            // If <path> name isn’t in the cache then
            // insert it and look it up again.
            insert_i(path);
            file_pointer = check_cache_i(path);

            // The calls to implementation methods
            // <insert_i> and <check_cache_i> assume
            // that the lock is held and perform work.
        }
        return file_pointer;
    }

    const void *check_cache_i(const string &) const
    { /* */
    }

    void insert_i(const string &)
    { /* … */
    }

    // … other private methods and data omitted …
};
					</script></code></pre>

					<aside class="notes" data-markdown>
						### Benefits:
						1. **Increased robustness**: this pattern ensures that self-deadlock does not occur due to
						intra-component method calls
						2. **Enhanced performance**: this pattern ensures locks are not acquired or released
						unnecessarily
						3. **Simplification of software**: by separating the locking and functionality concerns
					</aside>
				</section>

				<section>
					<h2 class="!text-[#de411b]">Double Checked Locking Optimization Design Pattern</h2>

					<p>Also known as <span class="text-[#b5533c]">Lock-Hint</span></p>

					<aside class="notes" data-markdown>
						## Double-Checked Locking Optimization Pattern

						- is used to reduce contention and synchronization overhead whenever critical sections of code
						must
						acquire locks in a thread-safe manner only once during program execution.
					</aside>
				</section>
				<section>
					<h4 class="!text-[#de411b]">Singleton.h</h4>
					<pre><code class="language-c++" data-trim data-line-numbers="4-13,17-18|21-22" data-noescape><script type="text/template">
class Singleton
{
public:
	static Singleton *instance()
	{
		if (instance_ == nullptr)
		{
			// Enter critical section.
			instance_ = new Singleton();
			// Leave critical section.
		}
		return instance_;
	}

	void method_1();
	// Other methods omitted.
private:
	static Singleton *instance_ = nullptr;
};

// somewhere.cpp
Singleton::instance()->method_1();
						</script></code></pre>
					<aside class="notes" data-markdown>
						- We will consider the `Singleton` pattern for our example use case

						### Singletons
						- The `Singleton` pattern ensures a class has only one instance and provides a global access
						point to that instance.
						- The following code demonstrates how a singleton might be implemented in C++
						- Callers use the _static_ `instance()` method to retrieve a pointer to the **Singleton
						instance** and then invoke public methods:

						### The Problem:
						- With the above implementation it is possible for the `Singleton` constructor to be called
						multiple times if:
						1. Multiple pre-emptive threads invoke Singleton::instance() simultaneously before it is
						initialized
						2. Multiple threads execute the dynamic initialization of the Singleton constructor within the
						critical section.
						- At best this causes a memory leak and at worst it triggers a whole lot of undefined behaviour

						*pointers are atomic
					</aside>
				</section>

				<section>
					<h4 class="!text-[#de411b]">Lets apply what we have learnt...</h4>

					<pre><code class="language-c++" data-trim data-line-numbers="7-12,20" data-noescape><script type="text/template">
class Singleton
{
public:
	static Singleton *instance()
	{
		if (instance_ == nullptr)
		{
			// Use Scoped Locking to acquire and
			// release <singleton_lock_> automatically.
			Guard<Thread_Mutex> guard(singleton_lock_);
			instance_ = new Singleton();
		}
		return instance_;
	}

	void method_1();
	// Other methods omitted.
private:
	static Singleton *instance_ = nullptr;
	static Thread_Mutex singleton_lock_;
};

// somewhere.cpp
Singleton::instance()->method_1();
					</script></code></pre>

					<p class="fragment"><span class="text-[#b5533c]">DO NOT</span> DO THIS.</p>
					<p class="fragment">...or maybe <a
							href="https://stackoverflow.com/questions/2576022/efficient-thread-safe-singleton-in-c">do</a>?
					</p>

					<aside class="notes" data-markdown>
						- So lets apply what we have learnt
						- Let's protect the critical section with a mutex
					</aside>
				</section>

				<section>
					<h4 class="!text-[#de411b]">Recommended Singleton</h4>

					<pre><code class="language-c++" data-trim data-line-numbers="7-15,23" data-noescape><script type="text/template">
class Singleton
{
public:
	static Singleton *instance()
	{
		if (instance_ == nullptr)
		{
			// Use Scoped Locking to acquire and
			// release <singleton_lock_> automatically.
			Guard<Thread_Mutex> guard(singleton_lock_);
			
			// Double check!!
			if (instance == nullptr)
				instance_ = new Singleton();
		}
		return instance_;
	}

	void method_1();
	// Other methods omitted.
private:
	static Singleton *instance_ = nullptr;
	static Thread_Mutex singleton_lock_;
};

// somewhere.cpp
Singleton::instance()->method_1();
					</script></code></pre>

					<p>Implement the <span class="text-[#b5533c]">first-in-time</span> flag.</p>

					<aside class="notes" data-markdown>
						- What is the recommended solution?
						- the solution here is to implement a `first-time-in` flag.
						- Basically, in this case, once the lock is acquired, recheck that the _instance_ has not been instantiated yet.

						## Discussion:
						- So why do we not need to do this potentially? ... well you tell me
					</aside>
				</section>
				<!-- END synchronization patterns -->
			</section>

			<section>
				<section>
					<h4 class="!text-[#de411b]">What can I do now?</h4>

					<p class="fragment flex justify-center"><a class="!text-[#379BD7]"
							href="https://github.com/cherbie/concurrency-primitives-in-action">
							<svg class="h-[72px] w-[72px]" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"
								fill="#fff" width="16" height="16">
								<path
									d="M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z">
								</path>
							</svg>
						</a></p>
				</section>

				<section data-background-iframe="https://github.com/cherbie/concurrency-primitives-in-action"></section>
			</section>
		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/zoom/zoom.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/search/search.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// Also available as an ES module, see:
		// https://revealjs.com/initialization/
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,
			width: 1200,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [
				RevealZoom,
				RevealNotes,
				RevealSearch,
				RevealMarkdown,
				RevealHighlight,
			],
		});
	</script>
	<script src="https://cdn.tailwindcss.com"></script>
</body>

</html>